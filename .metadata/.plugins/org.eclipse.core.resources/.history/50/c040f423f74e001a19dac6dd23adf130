'''
Created on 14-Feb-2020

@author: debanjandhar
'''

import hashlib
import logging
import os
import time

from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
import oci

from src.com.oracle.odc.utils.dataflow_client import DataFlowClient


# reference: carter's code: https://gist.github.com/cartershanklin/edcc3723cef5426239d43304be5603ce
class DataFlowOperator(BaseOperator):
    template_fields = ('executable', 'compartment_id', 'parameters', 'class_name', 'conf')
    ui_color = '#A6E6A6'

    @apply_defaults
    def __init__(
            self,
            sparkConfig,
            mainClassName,
            jarPath,
            dateToRun,
            compartment_id,
            executable,
            isClusterMode=True,
            isDevEnv=False,
            class_name=None,
            num_executors=1,
            inst_type="VM.Standard2.1",
            *args, **kwargs):

        super(DataFlowOperator, self).__init__(*args, **kwargs)
        self.sparkConfig = sparkConfig
        self.mainClassName = mainClassName
        self.jarPath = jarPath
        self.dateToRun = dateToRun
        self.executable = executable
        self.lang = 'PYTHON' if self.executable.endswith('.py') else 'JAVA'
        self.class_name = class_name
        self.compartment_id = compartment_id
        self.isClusterMode = isClusterMode
        self.isDevEnv = isDevEnv
        self.dataflow_client = DataFlowClient(
            compartment_id=self.compartment_id
        )

        config = oci.config.from_file()

        self.object_storage_client = oci.object_storage.ObjectStorageClient(config)
        self.namespace = self.object_storage_client.get_namespace().data
        self.bucket_name = "dataflow-userdata"
        self.num_executors = num_executors
        self.inst_type = inst_type

    def execute(self, context):
        
        # Create hash from executable. 
        # NOTE : this hash is set as display name and is used to identify if application with this display name was created before or not. 
#         if self.executable.startswith("oci://"):
#             script_hash = hashlib.md5(self.executable.encode('utf-8')).hexdigest()
#             script_os_path = self.executable
# 
#             object_name = self.executable
# 
#         else:
#             # If the file is present in local, create hash using its content. And then use this hash to identify if it was created before. 
#             script_hash = hashlib.md5(open(self.executable, "rb").read()).hexdigest()
# 
#             object_name = "executables/%s.py" % script_hash
# 
#             # Copy the local file into OCI Object Storage for Data flow to read. 
#             self.object_storage_client.put_object(
#                 self.namespace, self.bucket_name, object_name, open(self.executable).read())
# 
#             script_os_path = "oci://{bucket_name}@{namespace}/{object_name}".format(
#                 bucket_name=self.bucket_name,
#                 namespace=self.namespace,
#                 object_name=object_name)

        driver_class = self.mainClassName.rsplit('.', 1)[1]

        existing_app = None
        for app in self.dataflow_client.get_applications():
            logging.info("DataflowOperator says app is {}".format(app))
            
            #  Check if we have created an application with same name or not. 
            if app["displayName"] == driver_class:
                existing_app = app
                break

        # If application is not created in Data Flow, then create one. 
        if not existing_app:
            existing_app = self.dataflow_client.create_application(
                driver_class,
                self.jarPath,
                class_name=self.mainClassName,
                lang=self.lang,
                params=self.parameters,
                conf=self.conf,
                num_executors=self.num_executors,
                executor_shape=self.inst_type,
                driver_shape=self.inst_type,
            )

        application_id = existing_app["id"]

        task_name = context['task'].task_id + '_' + context['execution_date'].isoformat()
        run = self.dataflow_client.create_run(
            application_id,
            task_name,
            self.parameters,
            num_executors=self.num_executors,
            executor_shape=self.inst_type,
            driver_shape=self.inst_type,

        )

        logging.info(run)

        while True:
            logging.info('sleeping for 30 seconds...')
            time.sleep(30)

            runs = self.dataflow_client.get_runs()

            current_run = [x for x in runs if x['id'] == run['id']]

            state = current_run[0]['lifecycleState']

            logging.info('run %s state %s' % (current_run[0]['id'], state))

            if state not in ['ACCEPTED', 'IN_PROGRESS']:
                break

        if state == 'FAILED':
            logging.error(self.dataflow_client.get_stdout_logs(run['id']))
            
    def constructSparkConfAndUserParams(self, sparkConfig, driver_class, jarPath, dateToRun, isClusterMode, isDevEnv=False):
        params = {}
        confs = {}
        
        # These values will be directly set to dataflow client params. 
#         if isClusterMode == True:
#             params = sparkConfig.cluster_conf.items()
#         else:
#             params = sparkConfig.cluster_client_conf.items()
            
        params = sparkConfig.spark_conf.items()
        
        try:
            if isDevEnv:
                driver_class += "_dev"
                logging.info("driver_class = " + driver_class)
                confs = sparkConfig.driver_conf[driver_class].items()
        except KeyError:
            pass
        
        confs[' --startDate '] = dateToRun
        confs[' --endDate '] = dateToRun
        
        return params, confs
