package com.oracle.odc.metis.config;

import com.oracle.odc.metis.engine.*;
import com.oracle.odc.metis.jsonparse.*;
import com.oracle.odc.metis.utils.*;

import java.io.File;
import java.math.BigDecimal;
import java.sql.Date;
import java.sql.Timestamp;
import java.text.*;
import java.util.*;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.apache.spark.sql.AnalysisException;
import org.apache.spark.sql.Column;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.*;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.api.java.function.*;
import org.apache.spark.sql.Row;
import org.apache.spark.SparkContext;
import org.apache.spark.SparkException;
import static org.apache.spark.sql.functions.lit;
import static org.apache.spark.sql.functions.regexp_replace;

public class ConfigInterpreter {
	private static final Logger logger = LogManager.getLogger(ConfigInterpreter.class);

	protected ParsedConfig parsedConfig;
	protected List<String> tableNames;
	protected HashMap<String, Dataset> dfMap;
	protected String currentDate;

	protected static SparkSession sparkSession = null;
	protected static String env = null;
	protected static HashMap<String, DataType> str2TypeMap = null;

	public ConfigInterpreter(ParsedConfig configParser, String env, String currentDate) {
		this.parsedConfig = configParser;
		ConfigInterpreter.env = env;
		this.dfMap = new HashMap<String, Dataset>();
		this.tableNames = new ArrayList<String>();
		this.currentDate = currentDate;
		createStr2TypeMap();
	}

	/*
	 * PLEASE read before you add code to this method!!! Add your code to this
	 * method ONLY if you will introduce: 1) a new config section in config json AND
	 * 2)an action that is generic and can be applied to all config files/jobs
	 * Otherwise, please do the following steps: 1. provide a unique configName in
	 * your json config file, example config file:
	 * src/main/resouces/prod/audience_core_legacy.json 2. create a new subclass
	 * which extends this ConfigInterpreter class and add your custom code there,
	 * Example subclass: UwfConfigInterpreter.java. 3. add a case in
	 * ConfigInterpreterFactory.java 4. now you are ready to use
	 * ConfigInterpreterFactory to instantiate your custom config interpreter class!
	 */

	public void interpret() {
		try {
			// interpret read config fields
			if (!isReadSectionSpecified()) {
				logger.error("read section is not sepecified in config.");
				return;
			} else
				read();

			// interpret transpose fields
			if (isTransposeSectionSpecified(parsedConfig)) {
				logger.info("INFO: interpreting transpose section..");
				interpretTranspose(parsedConfig.getTranspose());
			}

			if (isJoinSectionSpecified()) {
				logger.info("INFO: interpreting join section..");
				join();

			}

			// interpret diff

			if (isDiffSectionSpecified() && isJoinSectionSpecified()) {
				logger.info("INFO: interpreting diff section..");
				interpretDiff(parsedConfig.getJoin().get(0), parsedConfig.getDiff());
			}

			// interpret lookup
			if (isLookupSectionSpecified()) {
				logger.info("INFO: interpreting lookup section..");
				interpretLookup(parsedConfig.getLookup());
			}

			// interpret weightings section for modeling files
			if (isWeightingsSectionSpecified()) {
				logger.info("INFO: interpreting weightings section..");
				interpretWeightings();

			}

			logger.info("INFO: write output..");
			write();

		} catch (ConfigInterpretException cie) {
			logger.error("Exception caught: " + cie.getErrorMsg());

		} finally {

			// drop dataframes
			logger.info("INFO: dropping all data frames..");
			dropDataframes();
		}
	}

	private void createStr2TypeMap() {
		str2TypeMap = new HashMap<String, DataType>();
		str2TypeMap.put(Constants.TYPE_STRING, DataTypes.StringType);
		str2TypeMap.put(Constants.TYPE_INTEGER, DataTypes.IntegerType);
		str2TypeMap.put(Constants.TYPE_BINARY, DataTypes.BinaryType);
		str2TypeMap.put(Constants.TYPE_BOOLEAN, DataTypes.BooleanType);
		str2TypeMap.put(Constants.TYPE_DOUBLE, DataTypes.DoubleType);
		str2TypeMap.put(Constants.TYPE_DATE, DataTypes.DateType);
		str2TypeMap.put(Constants.TYPE_FLOAT, DataTypes.FloatType);
		str2TypeMap.put(Constants.TYPE_TIMESTAMP, DataTypes.TimestampType);
		str2TypeMap.put(Constants.TYPE_CALENDAR_INTERVAL, DataTypes.CalendarIntervalType);
		str2TypeMap.put(Constants.TYPE_BYTE, DataTypes.ByteType);
		str2TypeMap.put(Constants.TYPE_LONG, DataTypes.LongType);
		str2TypeMap.put(Constants.TYPE_STRING_MULTIPLE_LINES, DataTypes.StringType);
		str2TypeMap.put(Constants.TYPE_DECIMAL, DataTypes.createDecimalType(12, 10));
	}

	protected static SparkSession getSparkSession() {
		if (sparkSession == null) {
			sparkSession = SparkUtils.createSparkSession(env);
		}
		SparkUtils.setLogLevel(sparkSession, Constants.ERROR);
		return sparkSession;
	}

	private boolean createDataframe(ReadConfigFields readConfigFields) {
		String tableName = readConfigFields.getTableName();
		List<String> uniqueKeys = readConfigFields.getUniqueKeys();
		String importFileFormat = readConfigFields.getImportFileFormat();
		String importFileLocation = readConfigFields.getImportFileLocation();
		String delim = readConfigFields.getImportDelimeter();
		List<ColumnField> importFields = readConfigFields.getImportFields();
		String header = readConfigFields.getHeader();
		if (header == null)
			header = "false";
		boolean hasCompositeId = hasCompositeId(importFields);

		List<ColumnInfo> columnInfoList = new ArrayList<ColumnInfo>();
		List<ColumnInfo> tempColumnInfoAsAllStringList = new ArrayList<>();
		try {
			for (int i = 0; i < importFields.size(); i++) {
				String type = importFields.get(i).getColType();
				if (str2TypeMap.containsKey(type)) {
					String colName = importFields.get(i).getColName();
					String newColName = importFields.get(i).getNewColName();
					ColumnInfo info = null;
					// Convert "value" column into StringType since input data
					// of
					// table "audience_attribute"
					// may contain mix of DataType, StringType and numeric type,
					// which will crash IE.
					if (isTransposeSectionSpecified(parsedConfig)
							&& tableName.contains(parsedConfig.getTranspose().getSparkTableName())
							&& colName.contains("value")) {
						info = new ColumnInfo(colName, newColName, str2TypeMap.get("StringType"));
					} else {
						info = new ColumnInfo(colName, newColName, str2TypeMap.get(type));
					}
					tempColumnInfoAsAllStringList
							.add(new ColumnInfo(colName, newColName, str2TypeMap.get("StringType")));
					columnInfoList.add(info);
				} else {
					logger.error("Failed to create Dataset. Wrong types");
					return false;
				}
			}
		} catch (Exception e) {
			e.printStackTrace();
		}

		try {

			Dataset input = FileUtils.createAndValidateDataset(getSparkSession(), tempColumnInfoAsAllStringList,
					columnInfoList, importFileLocation, importFileFormat, delim, header);

			if (input == null) {
				logger.error("Error: a null dataframe is created.");
				return false;
			}

			// remove newline characters from column values
			for (int i = 0; i < importFields.size(); i++) {
				String colName = importFields.get(i).getColName();
				String type = importFields.get(i).getColType();
				if (type.equals(Constants.TYPE_STRING_MULTIPLE_LINES)) {
					input = input.withColumn(colName, regexp_replace(input.col(colName), "\n", " "));
				}
			}
			String[] columnNames = input.columns();

			final List<String> list = new ArrayList<String>();
			Collections.addAll(list, columnNames);

			list.remove("version_prev");
			columnNames = list.toArray(new String[list.size()]);

			Dataset uniqueDataset = input.dropDuplicates(columnNames);

			logger.info("creating dataset..");

			System.out.println("tableName:" + tableName);
			uniqueDataset.show(20);
			SparkUtils.createTempTable(uniqueDataset, tableName);
			dfMap.put(tableName, uniqueDataset);

			logger.info("creating dataset '" + tableName);
			if (logger.isDebugEnabled())
				uniqueDataset.show(20);

		} catch (IllegalArgumentException e) {
			e.printStackTrace();
			logger.error("IllegalArgumentException when creating dataset: " + e);
			return false;
		} catch (Exception e) {
			e.printStackTrace();
			logger.error("Exception when creating dataset: " + e);
			return false;
		}

		return true;
	}

	private boolean hasCompositeId(List<ColumnField> importFields) {
		if (importFields == null || importFields.isEmpty())
			return false;
		for (ColumnField columnField : importFields) {
			if (columnField.getColName().equals("composite_id"))
				return true;
		}
		return false;
	}

	private boolean hasCHubVersion(List<ColumnField> importFields) {
		if (importFields == null || importFields.isEmpty())
			return false;
		for (ColumnField columnField : importFields) {
			if (columnField.getColName().equals("chub_version"))
				return true;
		}
		return false;
	}

	private boolean joinDataframes(JoinConfig joinConfig) {
		List<String> partitionKeys = joinConfig.getPartitionBy();
		String query = joinConfig.getQuery();
		String resultTableName = joinConfig.getResultTableName();

		// TODO partition before join in Q2
		// if (!partition(partitionKeys))
		// return false;

		Dataset result;
		try {
			result = SparkUtils.join(getSparkSession(), query);
		} catch (AnalysisException | SparkException e) {
			logger.error(e);
			return false;
		} catch (Exception e) {
			logger.error(e);
			return false;
		}
		logger.info("join result...");
		System.out.println("resultTableName:" + resultTableName);

		if (logger.isDebugEnabled())
			result.show(20);

		result = result.distinct();

		try {
			SparkUtils.createTempTable(result, resultTableName);
			dfMap.put(resultTableName, result);
		} catch (IllegalArgumentException e) {
			logger.error("Illegal argument exception when creating temp table: " + e);
			return false;
		} catch (Exception e) {
			logger.error("Error when creating temp table: " + e);
			return false;
		}
		return true;
	}

	// Assumption: each table only has one partition key, and order of
	// the partition keys in "partitionBy" in the config file matches
	// the order of tables specified in the "Read" section of the config file.
	private boolean partition(List<String> partitionKeys) {
		try {
			if (tableNames.size() != partitionKeys.size()) {
				logger.warn("Please specify right number of partition keys.");
				return false;
			}
			for (int i = 0; i < partitionKeys.size(); i++) {
				if (partitionKeys.get(i) == "")
					continue;
				String tableName = tableNames.get(i);
				Dataset partitionedDf = SparkUtils.partition(dfMap.get(tableName), partitionKeys.get(i));
				SparkUtils.dropTempTable(getSparkSession(), tableName);
				SparkUtils.createTempTable(partitionedDf, tableName);
			}
		} catch (Exception e) {
			logger.error(e);
			return false;
		}

		return true;
	}

	protected void interpretTranspose(TransposeConfig tranposeConfig) throws ConfigInterpretException {
		String tableName = tranposeConfig.getSparkTableName();
		List<String> groupByColumns = tranposeConfig.getGroupByColumns();
		String keyColumn = tranposeConfig.getKeyColumn();
		String valueColumn = tranposeConfig.getValueColumn();

		Dataset df = dfMap.get(tableName);
		if (df == null) {
			throw new ConfigInterpretException("Could not find table for transpose.");

		}

		Dataset transposedDf = null;
		try {
			transposedDf = Transpose.transpose(getSparkSession(), df, tableName, groupByColumns, keyColumn,
					valueColumn);
			if (transposedDf == null)
				throw new ConfigInterpretException("transposedDf is null.");
		} catch (Exception e) {
			e.printStackTrace();
			throw new ConfigInterpretException(e.getMessage());

		}
		dfMap.put(tableName, transposedDf);

	}

	protected void interpretDiff(JoinConfig joinConfig, DiffConfig diffConfig) throws ConfigInterpretException {
		String currentTable = diffConfig.getCurrentTable();
		String previousTable = diffConfig.getPreviousTable();
		String historicalTable = diffConfig.getHistoricalTable();
		String joinResult = diffConfig.getJoinResult();
		String creationDateRequired = diffConfig.getCreationDateRequired();
		String historyExportLocation = diffConfig.getHistoryExportLocation();
		String creationDateColFromPrevTable = diffConfig.getPreviousTableCreationDateColName();
		List<DiffColumns> columnsToBeDiffed = diffConfig.getColumnsToBeDiffed();

		if (!dfMap.containsKey(currentTable)) {
			throw new ConfigInterpretException(
					"Diff error: cannot find dataset for currentTable '" + currentTable + "'");

		}
		if (!dfMap.containsKey(previousTable)) {
			throw new ConfigInterpretException(
					"Diff error: cannot find dataset for previousTable '" + previousTable + "'");

		}
		if (!dfMap.containsKey(historicalTable)) {
			throw new ConfigInterpretException(
					"Diff error: cannot find dataset for historicalTable '" + historicalTable + "'");

		}

		if (!dfMap.containsKey(joinResult)) {
			throw new ConfigInterpretException("Diff error: cannot find dataset for joinResult '" + joinResult + "'");

		}
		Dataset currentDf = dfMap.get(currentTable);
		Dataset historicalDf = dfMap.get(historicalTable);
		Dataset joinResultDf = dfMap.get(joinResult);

		List<DiffColumnPair> diffColumns = new ArrayList<DiffColumnPair>();
		for (int i = 0; i < columnsToBeDiffed.size(); i++) {
			logger.info(columnsToBeDiffed.get(i).getPrevColName() + " : " + columnsToBeDiffed.get(i).getCurrColName());
			DiffColumnPair diffColPair = new DiffColumnPair(columnsToBeDiffed.get(i).getPrevColName(),
					columnsToBeDiffed.get(i).getCurrColName());
			diffColumns.add(diffColPair);
			logger.info(diffColPair.currColName + ":" + diffColPair.prevColName);
		}
		Dataset diffDf = null;
		try {
			Diff diffObj = new Diff(creationDateRequired.contains("true") ? true : false, this.currentDate);
			diffDf = diffObj.computeDiff(getSparkSession(), joinResultDf, currentDf, historicalDf,
					historyExportLocation, creationDateColFromPrevTable, diffColumns);
			if (diffDf == null)
				throw new ConfigInterpretException("error in compute diff, diffDf is null");

			logger.info("diffDf :");
			if (logger.isDebugEnabled())
				diffDf.show(20);

			dfMap.put(currentTable, diffDf);
			SparkUtils.createTempTable(diffDf, currentTable);

		} catch (Exception e) {
			e.printStackTrace();
			throw new ConfigInterpretException(e.getMessage());
		}

	}

	protected void interpretLookup(LookupConfig lookupConfig) throws ConfigInterpretException {
		try {
			String lookupTableName = lookupConfig.getLookupTableName();
			String lookupKeyColumn = lookupConfig.getLookupKeyColumn();
			String lookupValueColumn = lookupConfig.getLookupValueColumn();
			String tableNameForUpdate = lookupConfig.getTableNameForUpdate();
			String keyColumnFromUpdateTable = lookupConfig.getKeyColumnFromUpdateTable();
			String columnToBeUpdated = lookupConfig.getColumnToBeUpdated();

			Dataset left = dfMap.get(tableNameForUpdate);
			if (left == null) {
				throw new ConfigInterpretException("ERROR: Cannot find tableNameForUpdate in Lookup section.");

			}
			Dataset lookup = dfMap.get(lookupTableName);
			if (lookup == null) {
				throw new ConfigInterpretException("ERROR: Cannot find lookupTableName in Lookup section.");

			}

			CustomBroadcast cb = new CustomBroadcast();
			Dataset result = cb.lookupAndUpdate(getSparkSession(), left, lookup, keyColumnFromUpdateTable,
					columnToBeUpdated, lookupKeyColumn, lookupValueColumn);
			dfMap.put(tableNameForUpdate, result);

		} catch (Exception e) {
			e.printStackTrace();
			throw new ConfigInterpretException(e.getMessage());

		}

	}

	private void interpretJoinAndUpdate() throws ConfigInterpretException {
		List<JoinUpdateConfig> joinUpdateConfigs = parsedConfig.getJoinAndUpdate();
		for (JoinUpdateConfig joinUpdateConfig : joinUpdateConfigs) {
			String leftTable = joinUpdateConfig.getLeftTable();
			String leftTableKey = joinUpdateConfig.getLeftTableKey();
			String rightTable = joinUpdateConfig.getRightTable();
			String rightTableKey = joinUpdateConfig.getRightTableKey();
			String updateColName = joinUpdateConfig.getUpdateColName();
			String updatedTable = joinUpdateConfig.getUpdatedTable();
			String updateValue = joinUpdateConfig.getUpdateValue();

			try {
				Dataset left = dfMap.get(leftTable);
				if (left == null) {
					throw new ConfigInterpretException("ERROR: Cannot find leftTable in joinAndUpdate section.");

				}
				Dataset right = dfMap.get(rightTable);
				if (right == null) {
					throw new ConfigInterpretException("ERROR: Cannot find rightTable in joinAndUpdate section.");

				}

				CustomBroadcast cb = new CustomBroadcast();
				Dataset result = cb.joinAndUpdate(getSparkSession(), left, right, leftTableKey, updateColName,
						rightTableKey, updateValue);
				logger.info("Joinandupdate result is:");

				if (result == null) {
					logger.error("joinandupdate result is null");
					throw new ConfigInterpretException("ERROR: Cannot join and update table in joinAndUpdate section.");

				} else {

					logger.info("Result is not null");
					if (logger.isDebugEnabled())
						result.show(20);
				}

				/*
				 * left.show(); right.show(); result.show();
				 * result.select(updateColName).show();
				 */
				dfMap.put(updatedTable, result);

			} catch (Exception e) {
				logger.error(e);
				e.printStackTrace();
				throw new ConfigInterpretException(e.getMessage());

			}
		}

	}

	private void interpretWeightings() throws ConfigInterpretException {
		List<WeigthingsField> fields = parsedConfig.getWeightings();
		for (WeigthingsField field : fields) {
			String type = field.getPercentageType();
			List<String> inputTables = field.getInputTable();
			List<String> outputTables = field.getOutputTable();
			try {
				if (type.equals(Constants.WEIGHTINGS_PERCENTAGE_TYPE_SIMPLE)) {
					if (inputTables.size() == 0 || inputTables.size() == 1 && inputTables.get(0).equals("")
							|| outputTables.size() == 0 || outputTables.size() == 1 && outputTables.get(0).equals(""))
						continue; // continue on next field
					Dataset inputDf = dfMap.get(inputTables.get(0));
					if (inputDf == null) {
						throw new ConfigInterpretException("ERROR: Cannot find inputDf in weightings section.");

					}
					Dataset outputDf = Weightings.addWeightingsSimple(getSparkSession(), inputDf,
							Constants.COLUMN_NAME_MODELED_AUDIENCE_ID, Constants.COLUMN_NAME_WEIGHTING);
					dfMap.put(outputTables.get(0), outputDf);

				} else if (type.equals(Constants.WEIGHTINGS_PERCENTAGE_TYPE_COMPLEX)) {
					if (inputTables.size() < 2) {
						logger.error(
								"ERROR: In weightings section, need to specify both curated input and provider map input table names.");
						continue; // continue on next field
					}
					if (outputTables.size() < 2) {
						logger.error(
								"ERROR: In weightings section, need to specify both curated input and provider map output table names.");
						continue; // continue on next field
					}
					Dataset dfCuratedInput = dfMap.get(inputTables.get(0));
					Dataset dfProviderMap = dfMap.get(inputTables.get(1));
					WeightingCalculationComplex weightingCal = new WeightingCalculationComplex();
					HashMap<String, Dataset> res = weightingCal.calWeightingForCurated(getSparkSession(), dfProviderMap,
							dfCuratedInput);
					dfMap.put(outputTables.get(0), res.get("curated_recursive_result"));
					dfMap.put(outputTables.get(1), res.get("provider_map_result"));

				} else {
					logger.error("ERROR: In weightings section, percentageType '" + type + "' is not supported.");
					continue; // continue on next field
				}

			} catch (Exception e) {
				logger.error(e);
				e.printStackTrace();
				throw new ConfigInterpretException(e.getMessage());
			}
		}

	}

	private boolean writeConfig(WriteConfigFields writeConfig) {
		String tableName = writeConfig.getTableName();
		String exportFileFormat = writeConfig.getExportFileFormat();
		String exportFileLocation = writeConfig.getExportFileLocation();
		String exportFilename = writeConfig.getExportFilename();
		String exportDelimeter = writeConfig.getExportDelimeter();
		String coalesce = writeConfig.getCoalesce();
		String header = writeConfig.getHeader();

		Dataset df = dfMap.get(tableName);

		logger.info("Write table name: " + tableName);

		Dataset originalDataset = dfMap.get(tableName);

		if (originalDataset == null) {
			logger.warn("Unable to get Dataframe '" + tableName + "'");
			return false;
		}

		String[] columnNames = originalDataset.columns();

		final List<String> list = new ArrayList<String>();
		Collections.addAll(list, columnNames);

		// list.remove("version");
		// TODO Verify why dropDuplicates was added. Will it cause problems?
		columnNames = list.toArray(new String[list.size()]);
		Dataset uniqueDataset = originalDataset.dropDuplicates(columnNames);
		try {
			if (coalesce != null && coalesce.equalsIgnoreCase("true")) {
				FileUtils.persistDatasetWithCoalesce(uniqueDataset, exportFileLocation, exportFileFormat,
						exportDelimeter, Constants.OVERWRITE);
			} else if (header != null && header.equalsIgnoreCase("false")) {
				FileUtils.persistCoalesceWithoutHeader(uniqueDataset, exportFileLocation, exportFileFormat,
						exportDelimeter, Constants.OVERWRITE);
			} else {
				FileUtils.persistDataset(uniqueDataset, exportFileLocation, exportFileFormat, exportDelimeter,
						Constants.OVERWRITE);
			}
		} catch (IllegalArgumentException e) {
			logger.error(e);
			return false;
		} catch (Exception e) {

			logger.error(e);
			return false;
		}
		return true;
	}

	protected void dropDataframes() {
		if (dfMap != null && !dfMap.isEmpty()) {
			for (String tableName : dfMap.keySet()) {
				SparkUtils.dropTempTable(getSparkSession(), tableName);
			}
		}
	}

	protected boolean isReadSectionSpecified() {
		List<ReadConfigFields> readConfigFieldsList = parsedConfig.getRead();
		if (readConfigFieldsList == null) {
			logger.info("INFO: Read section is not specified.");
			return false;
		}
		for (int i = 0; i < readConfigFieldsList.size(); i++) {
			ReadConfigFields readConfig = readConfigFieldsList.get(i);
			if (readConfig == null) {
				logger.info("INFO: Read section item is not specified.");
				return false;
			}
			if (readConfig.getTableName() == "") {
				logger.info("INFO: Read section table is not specified.");
				return false;
			}
		}
		return true;
	}

	protected boolean isWriteSectionSpecified(ParsedConfig configParser) {
		List<WriteConfigFields> writeConfigFieldsList = configParser.getWrite();
		if (writeConfigFieldsList == null) {
			logger.info("INFO: Write section is not specified.");
			return false;
		}
		for (int i = 0; i < writeConfigFieldsList.size(); i++) {
			WriteConfigFields writeConfig = writeConfigFieldsList.get(i);
			if (writeConfig == null) {
				logger.info("INFO: Write section item is not specified.");
				return false;
			}
			if (writeConfig.getTableName() == "") {
				logger.info("INFO: Write section table is not specified.");
				return false;
			}
		}
		return true;
	}

	protected boolean isJoinSectionSpecified() {
		List<JoinConfig> joinConfigs = parsedConfig.getJoin();
		if (joinConfigs == null) {
			logger.info("INFO: Join section is not specified.");
			return false;
		}
		for (int i = 0; i < joinConfigs.size(); i++) {
			JoinConfig joinConfig = joinConfigs.get(i);
			if (joinConfig == null) {
				logger.info("INFO: Join section item is not specified.");
				return false;
			}
			if (joinConfig.getQuery() == "") {
				logger.info("INFO: Join section query is not specified.");
				return false;
			}
		}
		return true;
	}

	private boolean isTransposeSectionSpecified(ParsedConfig configParser) {
		TransposeConfig transposeConfig = configParser.getTranspose();
		if (transposeConfig == null) {
			logger.info("INFO: Transpose section is not specified.");
			return false;
		}
		if (transposeConfig.getSparkTableName() == "") {
			logger.info("INFO: Transpose section table is not specified.");
			return false;
		}
		return true;
	}

	protected boolean isDiffSectionSpecified() {
		DiffConfig diffConfig = parsedConfig.getDiff();
		if (diffConfig == null) {
			logger.info("INFO: Diff section is not specified.");
			return false;
		}
		if (diffConfig.getCurrentTable() == "") {
			logger.info("INFO: Diff section table is not specified.");
			return false;
		}
		return true;
	}

	protected boolean isLookupSectionSpecified() {
		LookupConfig lookupConfig = parsedConfig.getLookup();
		if (lookupConfig == null) {
			logger.info("INFO: Lookup section is not specified.");
			return false;
		}
		if (lookupConfig.getLookupTableName() == "") {
			logger.info("INFO: Lookup table is not specified.");
			return false;
		}
		return true;
	}

	protected boolean isJoinAndUpdateSectionSpecified() {
		List<JoinUpdateConfig> joinUpdateConfigs = parsedConfig.getJoinAndUpdate();
		if (joinUpdateConfigs == null) {
			logger.info("INFO: joinAndUpdate section is not specified.");
			return false;
		}
		for (JoinUpdateConfig joinUpdateConfig : joinUpdateConfigs) {
			if (joinUpdateConfig == null || joinUpdateConfig.getLeftTable() == "") {
				logger.info("INFO: joinAndUpdate table is not specified.");
				return false;
			}
		}
		return true;
	}

	protected boolean isWeightingsSectionSpecified() {
		List<WeigthingsField> fields = parsedConfig.getWeightings();
		if (fields == null) {
			logger.info("INFO: Weightings section is not specified.");
			return false;
		}
		for (WeigthingsField field : fields) {
			if (field == null || field.getPercentageType() == "") {
				logger.info("INFO: Weightings table is not specified.");
				return false;
			}
		}
		return true;
	}

	protected void read() throws ConfigInterpretException {

		List<ReadConfigFields> readConfigFieldsList = parsedConfig.getRead();
		for (int i = 0; i < readConfigFieldsList.size(); i++) {
			ReadConfigFields readConfig = readConfigFieldsList.get(i);
			tableNames.add(readConfig.getTableName());
			if (!createDataframe(readConfig)) {

				throw new ConfigInterpretException("Exception caught in Read Section.");
			}
		}

	}

	protected void write() throws ConfigInterpretException {
		if (isWriteSectionSpecified(parsedConfig)) {
			List<WriteConfigFields> writeConfigFieldsList = parsedConfig.getWrite();
			for (int i = 0; i < writeConfigFieldsList.size(); i++) {
				WriteConfigFields writeConfig = writeConfigFieldsList.get(i);
				if (!writeConfig(writeConfig)) {
					throw new ConfigInterpretException("Exception caught in Write Section.");
				}
			}
		}
	}

	protected void join() throws ConfigInterpretException {
		join(null);
	}

	protected void join(Integer sequenceId) throws ConfigInterpretException {
		List<JoinConfig> joinList = parsedConfig.getJoin();
		for (int i = 0; i < joinList.size(); i++) {
			JoinConfig join_config = joinList.get(i);
			if (!joinDataframes(join_config)) {
				throw new ConfigInterpretException("Exception caught in Join Section.");

			}
		}
	}

	private boolean checkSequenceIdInJoinList(JoinConfig joinConfig, Integer sequenceId) {

		// Sequnence Id should be >=1
		if (sequenceId < 1) {
			logger.error("Sequence Id should be greater that 1. Ignoring Join Config with negative sequence id ["
					+ joinConfig.getSequenceId() + "] for [" + joinConfig.getQuery() + "]");
			return false;
		}

		
		
	}

	// For testing only
	private void dumpRead(List<ReadConfigFields> read) {
		for (int i = 0; i < read.size(); i++) {
			ReadConfigFields read_config_fields = read.get(i);
			String tableName = read_config_fields.getTableName();
			String importFileFormat = read_config_fields.getImportFileFormat();
			String importFileLocation = read_config_fields.getImportFileLocation();
			String delim = read_config_fields.getImportDelimeter();
			List<ColumnField> importFields = read_config_fields.getImportFields();
			logger.info("--Read Section--");
			logger.info("tableName: " + tableName);
			logger.info("importFileFormat: " + importFileFormat);
			logger.info("importFileLocation: " + importFileLocation);
			logger.info("delim: " + delim);
			logger.info("importFields:");
			for (int j = 0; j < importFields.size(); j++) {
				logger.info("  colName: " + importFields.get(j).getColName());
				logger.info("  colType: " + importFields.get(j).getColType());
			}
		}
	}

	private void dumpDfMap() {
		logger.info("Size of the dfMap: " + dfMap.size());
		for (String key : dfMap.keySet()) {
			logger.info("tableName: " + key + ", df: " + dfMap.get(key));
		}
	}
}
