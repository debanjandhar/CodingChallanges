package com.oracle.odc.metis.driver;
import javax.xml.crypto.Data;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;

import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;

public class ConvertParqToCSV {

	public static void main(String[] args) {

		SparkSession ss = createSparkSession("dev");

//		String readPath = "/Users/debanjandhar/workspace/softwares/DataFlow/input_files/part-00000-c8a69cad-144c-4fa8-9ef3-c329875febd8-c000.snappy.parquet";
//		String writePath = "/Users/debanjandhar/workspace/softwares/DataFlow/input_files/parqOutput";
//		String readPath = "/Users/debanjandhar/workspace/softwares/DataFlow/input_files/parqOutput";
		String readPath = "/Users/debanjandhar/Downloads/rate_cards";
		String writePath = "/Users/debanjandhar/Downloads/rate_cards/csv";

//		ss.read().parquet(readPath).limit(20).write().parquet(writePath);;
//		ss.read().parquet(readPath).show(30);
		
		ss.read().json(readPath).show();
		Dataset readData = ss.read().json(readPath);
		Dataset writeData = addWithColumn(readData, "advertisingTypes", "buyers", "categoryPrices");
		writeData.write().option("header", true).option("delimiter", ",").format("com.databricks.spark.csv").save(writePath);
//		ss.read().json(readPath).withColumn("ArrayOfString", functions.lit("ArrayOfString").cast("string")).write().option("header", true).option("delimiter", ",").format("com.databricks.spark.csv").save(writePath);

	}
	
	static Dataset addWithColumn(Dataset ds, String... args) {
		for (String string : args) {
			ds = ds.withColumn(string, functions.lit(string).cast("string"));
		}
		
		return ds;
	}
	
	public static SparkSession createSparkSession(String env) {
		Config appConfig = ConfigFactory.load();
		SparkSession sparkSession = SparkSession.builder().appName(appConfig.getString("spark.appName")).master("local")
				.getOrCreate();
		return sparkSession;
	}

}

